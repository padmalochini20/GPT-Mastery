{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38ae4905",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import names\n",
    "words = open('makemore/names.txt','r').read().splitlines() # split each line and create list of each lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "831fda29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d67709ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ec3f816",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = []           #inputs\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) +['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        x1.append(ch1+ch2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27a1d094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "601"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(x1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42b4e3ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.e', 'em', 'mm', 'ma', '.o']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84b7f5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "char2=['.']\n",
    "char2.extend(sorted(list(set(''.join(words)))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b93ffb0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af4b79a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "729"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of possible two character string to train a trigram model\n",
    "x1=[]\n",
    "\n",
    "for i in char2:\n",
    "    for j in char2:\n",
    "        x1.append(i+j)\n",
    "        \n",
    "len(x1)    # 27*27 = 729 possible characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "08f0845d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['..', '.a', '.b', '.c', '.d']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04bdd0d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(729, 27)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create string to interger dictionary stoi for input and output\n",
    "\n",
    "stoi = {s:i for i, s in enumerate(x1) }\n",
    "\n",
    "stoi2 = {s:i for i, s in enumerate(char2)}\n",
    "len(stoi),len(stoi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "58e69cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'..': 0,\n",
       " '.a': 1,\n",
       " '.b': 2,\n",
       " '.c': 3,\n",
       " '.d': 4,\n",
       " '.e': 5,\n",
       " '.f': 6,\n",
       " '.g': 7,\n",
       " '.h': 8,\n",
       " '.i': 9,\n",
       " '.j': 10,\n",
       " '.k': 11,\n",
       " '.l': 12,\n",
       " '.m': 13,\n",
       " '.n': 14,\n",
       " '.o': 15,\n",
       " '.p': 16,\n",
       " '.q': 17,\n",
       " '.r': 18,\n",
       " '.s': 19,\n",
       " '.t': 20,\n",
       " '.u': 21,\n",
       " '.v': 22,\n",
       " '.w': 23,\n",
       " '.x': 24,\n",
       " '.y': 25,\n",
       " '.z': 26,\n",
       " 'a.': 27,\n",
       " 'aa': 28,\n",
       " 'ab': 29,\n",
       " 'ac': 30,\n",
       " 'ad': 31,\n",
       " 'ae': 32,\n",
       " 'af': 33,\n",
       " 'ag': 34,\n",
       " 'ah': 35,\n",
       " 'ai': 36,\n",
       " 'aj': 37,\n",
       " 'ak': 38,\n",
       " 'al': 39,\n",
       " 'am': 40,\n",
       " 'an': 41,\n",
       " 'ao': 42,\n",
       " 'ap': 43,\n",
       " 'aq': 44,\n",
       " 'ar': 45,\n",
       " 'as': 46,\n",
       " 'at': 47,\n",
       " 'au': 48,\n",
       " 'av': 49,\n",
       " 'aw': 50,\n",
       " 'ax': 51,\n",
       " 'ay': 52,\n",
       " 'az': 53,\n",
       " 'b.': 54,\n",
       " 'ba': 55,\n",
       " 'bb': 56,\n",
       " 'bc': 57,\n",
       " 'bd': 58,\n",
       " 'be': 59,\n",
       " 'bf': 60,\n",
       " 'bg': 61,\n",
       " 'bh': 62,\n",
       " 'bi': 63,\n",
       " 'bj': 64,\n",
       " 'bk': 65,\n",
       " 'bl': 66,\n",
       " 'bm': 67,\n",
       " 'bn': 68,\n",
       " 'bo': 69,\n",
       " 'bp': 70,\n",
       " 'bq': 71,\n",
       " 'br': 72,\n",
       " 'bs': 73,\n",
       " 'bt': 74,\n",
       " 'bu': 75,\n",
       " 'bv': 76,\n",
       " 'bw': 77,\n",
       " 'bx': 78,\n",
       " 'by': 79,\n",
       " 'bz': 80,\n",
       " 'c.': 81,\n",
       " 'ca': 82,\n",
       " 'cb': 83,\n",
       " 'cc': 84,\n",
       " 'cd': 85,\n",
       " 'ce': 86,\n",
       " 'cf': 87,\n",
       " 'cg': 88,\n",
       " 'ch': 89,\n",
       " 'ci': 90,\n",
       " 'cj': 91,\n",
       " 'ck': 92,\n",
       " 'cl': 93,\n",
       " 'cm': 94,\n",
       " 'cn': 95,\n",
       " 'co': 96,\n",
       " 'cp': 97,\n",
       " 'cq': 98,\n",
       " 'cr': 99,\n",
       " 'cs': 100,\n",
       " 'ct': 101,\n",
       " 'cu': 102,\n",
       " 'cv': 103,\n",
       " 'cw': 104,\n",
       " 'cx': 105,\n",
       " 'cy': 106,\n",
       " 'cz': 107,\n",
       " 'd.': 108,\n",
       " 'da': 109,\n",
       " 'db': 110,\n",
       " 'dc': 111,\n",
       " 'dd': 112,\n",
       " 'de': 113,\n",
       " 'df': 114,\n",
       " 'dg': 115,\n",
       " 'dh': 116,\n",
       " 'di': 117,\n",
       " 'dj': 118,\n",
       " 'dk': 119,\n",
       " 'dl': 120,\n",
       " 'dm': 121,\n",
       " 'dn': 122,\n",
       " 'do': 123,\n",
       " 'dp': 124,\n",
       " 'dq': 125,\n",
       " 'dr': 126,\n",
       " 'ds': 127,\n",
       " 'dt': 128,\n",
       " 'du': 129,\n",
       " 'dv': 130,\n",
       " 'dw': 131,\n",
       " 'dx': 132,\n",
       " 'dy': 133,\n",
       " 'dz': 134,\n",
       " 'e.': 135,\n",
       " 'ea': 136,\n",
       " 'eb': 137,\n",
       " 'ec': 138,\n",
       " 'ed': 139,\n",
       " 'ee': 140,\n",
       " 'ef': 141,\n",
       " 'eg': 142,\n",
       " 'eh': 143,\n",
       " 'ei': 144,\n",
       " 'ej': 145,\n",
       " 'ek': 146,\n",
       " 'el': 147,\n",
       " 'em': 148,\n",
       " 'en': 149,\n",
       " 'eo': 150,\n",
       " 'ep': 151,\n",
       " 'eq': 152,\n",
       " 'er': 153,\n",
       " 'es': 154,\n",
       " 'et': 155,\n",
       " 'eu': 156,\n",
       " 'ev': 157,\n",
       " 'ew': 158,\n",
       " 'ex': 159,\n",
       " 'ey': 160,\n",
       " 'ez': 161,\n",
       " 'f.': 162,\n",
       " 'fa': 163,\n",
       " 'fb': 164,\n",
       " 'fc': 165,\n",
       " 'fd': 166,\n",
       " 'fe': 167,\n",
       " 'ff': 168,\n",
       " 'fg': 169,\n",
       " 'fh': 170,\n",
       " 'fi': 171,\n",
       " 'fj': 172,\n",
       " 'fk': 173,\n",
       " 'fl': 174,\n",
       " 'fm': 175,\n",
       " 'fn': 176,\n",
       " 'fo': 177,\n",
       " 'fp': 178,\n",
       " 'fq': 179,\n",
       " 'fr': 180,\n",
       " 'fs': 181,\n",
       " 'ft': 182,\n",
       " 'fu': 183,\n",
       " 'fv': 184,\n",
       " 'fw': 185,\n",
       " 'fx': 186,\n",
       " 'fy': 187,\n",
       " 'fz': 188,\n",
       " 'g.': 189,\n",
       " 'ga': 190,\n",
       " 'gb': 191,\n",
       " 'gc': 192,\n",
       " 'gd': 193,\n",
       " 'ge': 194,\n",
       " 'gf': 195,\n",
       " 'gg': 196,\n",
       " 'gh': 197,\n",
       " 'gi': 198,\n",
       " 'gj': 199,\n",
       " 'gk': 200,\n",
       " 'gl': 201,\n",
       " 'gm': 202,\n",
       " 'gn': 203,\n",
       " 'go': 204,\n",
       " 'gp': 205,\n",
       " 'gq': 206,\n",
       " 'gr': 207,\n",
       " 'gs': 208,\n",
       " 'gt': 209,\n",
       " 'gu': 210,\n",
       " 'gv': 211,\n",
       " 'gw': 212,\n",
       " 'gx': 213,\n",
       " 'gy': 214,\n",
       " 'gz': 215,\n",
       " 'h.': 216,\n",
       " 'ha': 217,\n",
       " 'hb': 218,\n",
       " 'hc': 219,\n",
       " 'hd': 220,\n",
       " 'he': 221,\n",
       " 'hf': 222,\n",
       " 'hg': 223,\n",
       " 'hh': 224,\n",
       " 'hi': 225,\n",
       " 'hj': 226,\n",
       " 'hk': 227,\n",
       " 'hl': 228,\n",
       " 'hm': 229,\n",
       " 'hn': 230,\n",
       " 'ho': 231,\n",
       " 'hp': 232,\n",
       " 'hq': 233,\n",
       " 'hr': 234,\n",
       " 'hs': 235,\n",
       " 'ht': 236,\n",
       " 'hu': 237,\n",
       " 'hv': 238,\n",
       " 'hw': 239,\n",
       " 'hx': 240,\n",
       " 'hy': 241,\n",
       " 'hz': 242,\n",
       " 'i.': 243,\n",
       " 'ia': 244,\n",
       " 'ib': 245,\n",
       " 'ic': 246,\n",
       " 'id': 247,\n",
       " 'ie': 248,\n",
       " 'if': 249,\n",
       " 'ig': 250,\n",
       " 'ih': 251,\n",
       " 'ii': 252,\n",
       " 'ij': 253,\n",
       " 'ik': 254,\n",
       " 'il': 255,\n",
       " 'im': 256,\n",
       " 'in': 257,\n",
       " 'io': 258,\n",
       " 'ip': 259,\n",
       " 'iq': 260,\n",
       " 'ir': 261,\n",
       " 'is': 262,\n",
       " 'it': 263,\n",
       " 'iu': 264,\n",
       " 'iv': 265,\n",
       " 'iw': 266,\n",
       " 'ix': 267,\n",
       " 'iy': 268,\n",
       " 'iz': 269,\n",
       " 'j.': 270,\n",
       " 'ja': 271,\n",
       " 'jb': 272,\n",
       " 'jc': 273,\n",
       " 'jd': 274,\n",
       " 'je': 275,\n",
       " 'jf': 276,\n",
       " 'jg': 277,\n",
       " 'jh': 278,\n",
       " 'ji': 279,\n",
       " 'jj': 280,\n",
       " 'jk': 281,\n",
       " 'jl': 282,\n",
       " 'jm': 283,\n",
       " 'jn': 284,\n",
       " 'jo': 285,\n",
       " 'jp': 286,\n",
       " 'jq': 287,\n",
       " 'jr': 288,\n",
       " 'js': 289,\n",
       " 'jt': 290,\n",
       " 'ju': 291,\n",
       " 'jv': 292,\n",
       " 'jw': 293,\n",
       " 'jx': 294,\n",
       " 'jy': 295,\n",
       " 'jz': 296,\n",
       " 'k.': 297,\n",
       " 'ka': 298,\n",
       " 'kb': 299,\n",
       " 'kc': 300,\n",
       " 'kd': 301,\n",
       " 'ke': 302,\n",
       " 'kf': 303,\n",
       " 'kg': 304,\n",
       " 'kh': 305,\n",
       " 'ki': 306,\n",
       " 'kj': 307,\n",
       " 'kk': 308,\n",
       " 'kl': 309,\n",
       " 'km': 310,\n",
       " 'kn': 311,\n",
       " 'ko': 312,\n",
       " 'kp': 313,\n",
       " 'kq': 314,\n",
       " 'kr': 315,\n",
       " 'ks': 316,\n",
       " 'kt': 317,\n",
       " 'ku': 318,\n",
       " 'kv': 319,\n",
       " 'kw': 320,\n",
       " 'kx': 321,\n",
       " 'ky': 322,\n",
       " 'kz': 323,\n",
       " 'l.': 324,\n",
       " 'la': 325,\n",
       " 'lb': 326,\n",
       " 'lc': 327,\n",
       " 'ld': 328,\n",
       " 'le': 329,\n",
       " 'lf': 330,\n",
       " 'lg': 331,\n",
       " 'lh': 332,\n",
       " 'li': 333,\n",
       " 'lj': 334,\n",
       " 'lk': 335,\n",
       " 'll': 336,\n",
       " 'lm': 337,\n",
       " 'ln': 338,\n",
       " 'lo': 339,\n",
       " 'lp': 340,\n",
       " 'lq': 341,\n",
       " 'lr': 342,\n",
       " 'ls': 343,\n",
       " 'lt': 344,\n",
       " 'lu': 345,\n",
       " 'lv': 346,\n",
       " 'lw': 347,\n",
       " 'lx': 348,\n",
       " 'ly': 349,\n",
       " 'lz': 350,\n",
       " 'm.': 351,\n",
       " 'ma': 352,\n",
       " 'mb': 353,\n",
       " 'mc': 354,\n",
       " 'md': 355,\n",
       " 'me': 356,\n",
       " 'mf': 357,\n",
       " 'mg': 358,\n",
       " 'mh': 359,\n",
       " 'mi': 360,\n",
       " 'mj': 361,\n",
       " 'mk': 362,\n",
       " 'ml': 363,\n",
       " 'mm': 364,\n",
       " 'mn': 365,\n",
       " 'mo': 366,\n",
       " 'mp': 367,\n",
       " 'mq': 368,\n",
       " 'mr': 369,\n",
       " 'ms': 370,\n",
       " 'mt': 371,\n",
       " 'mu': 372,\n",
       " 'mv': 373,\n",
       " 'mw': 374,\n",
       " 'mx': 375,\n",
       " 'my': 376,\n",
       " 'mz': 377,\n",
       " 'n.': 378,\n",
       " 'na': 379,\n",
       " 'nb': 380,\n",
       " 'nc': 381,\n",
       " 'nd': 382,\n",
       " 'ne': 383,\n",
       " 'nf': 384,\n",
       " 'ng': 385,\n",
       " 'nh': 386,\n",
       " 'ni': 387,\n",
       " 'nj': 388,\n",
       " 'nk': 389,\n",
       " 'nl': 390,\n",
       " 'nm': 391,\n",
       " 'nn': 392,\n",
       " 'no': 393,\n",
       " 'np': 394,\n",
       " 'nq': 395,\n",
       " 'nr': 396,\n",
       " 'ns': 397,\n",
       " 'nt': 398,\n",
       " 'nu': 399,\n",
       " 'nv': 400,\n",
       " 'nw': 401,\n",
       " 'nx': 402,\n",
       " 'ny': 403,\n",
       " 'nz': 404,\n",
       " 'o.': 405,\n",
       " 'oa': 406,\n",
       " 'ob': 407,\n",
       " 'oc': 408,\n",
       " 'od': 409,\n",
       " 'oe': 410,\n",
       " 'of': 411,\n",
       " 'og': 412,\n",
       " 'oh': 413,\n",
       " 'oi': 414,\n",
       " 'oj': 415,\n",
       " 'ok': 416,\n",
       " 'ol': 417,\n",
       " 'om': 418,\n",
       " 'on': 419,\n",
       " 'oo': 420,\n",
       " 'op': 421,\n",
       " 'oq': 422,\n",
       " 'or': 423,\n",
       " 'os': 424,\n",
       " 'ot': 425,\n",
       " 'ou': 426,\n",
       " 'ov': 427,\n",
       " 'ow': 428,\n",
       " 'ox': 429,\n",
       " 'oy': 430,\n",
       " 'oz': 431,\n",
       " 'p.': 432,\n",
       " 'pa': 433,\n",
       " 'pb': 434,\n",
       " 'pc': 435,\n",
       " 'pd': 436,\n",
       " 'pe': 437,\n",
       " 'pf': 438,\n",
       " 'pg': 439,\n",
       " 'ph': 440,\n",
       " 'pi': 441,\n",
       " 'pj': 442,\n",
       " 'pk': 443,\n",
       " 'pl': 444,\n",
       " 'pm': 445,\n",
       " 'pn': 446,\n",
       " 'po': 447,\n",
       " 'pp': 448,\n",
       " 'pq': 449,\n",
       " 'pr': 450,\n",
       " 'ps': 451,\n",
       " 'pt': 452,\n",
       " 'pu': 453,\n",
       " 'pv': 454,\n",
       " 'pw': 455,\n",
       " 'px': 456,\n",
       " 'py': 457,\n",
       " 'pz': 458,\n",
       " 'q.': 459,\n",
       " 'qa': 460,\n",
       " 'qb': 461,\n",
       " 'qc': 462,\n",
       " 'qd': 463,\n",
       " 'qe': 464,\n",
       " 'qf': 465,\n",
       " 'qg': 466,\n",
       " 'qh': 467,\n",
       " 'qi': 468,\n",
       " 'qj': 469,\n",
       " 'qk': 470,\n",
       " 'ql': 471,\n",
       " 'qm': 472,\n",
       " 'qn': 473,\n",
       " 'qo': 474,\n",
       " 'qp': 475,\n",
       " 'qq': 476,\n",
       " 'qr': 477,\n",
       " 'qs': 478,\n",
       " 'qt': 479,\n",
       " 'qu': 480,\n",
       " 'qv': 481,\n",
       " 'qw': 482,\n",
       " 'qx': 483,\n",
       " 'qy': 484,\n",
       " 'qz': 485,\n",
       " 'r.': 486,\n",
       " 'ra': 487,\n",
       " 'rb': 488,\n",
       " 'rc': 489,\n",
       " 'rd': 490,\n",
       " 're': 491,\n",
       " 'rf': 492,\n",
       " 'rg': 493,\n",
       " 'rh': 494,\n",
       " 'ri': 495,\n",
       " 'rj': 496,\n",
       " 'rk': 497,\n",
       " 'rl': 498,\n",
       " 'rm': 499,\n",
       " 'rn': 500,\n",
       " 'ro': 501,\n",
       " 'rp': 502,\n",
       " 'rq': 503,\n",
       " 'rr': 504,\n",
       " 'rs': 505,\n",
       " 'rt': 506,\n",
       " 'ru': 507,\n",
       " 'rv': 508,\n",
       " 'rw': 509,\n",
       " 'rx': 510,\n",
       " 'ry': 511,\n",
       " 'rz': 512,\n",
       " 's.': 513,\n",
       " 'sa': 514,\n",
       " 'sb': 515,\n",
       " 'sc': 516,\n",
       " 'sd': 517,\n",
       " 'se': 518,\n",
       " 'sf': 519,\n",
       " 'sg': 520,\n",
       " 'sh': 521,\n",
       " 'si': 522,\n",
       " 'sj': 523,\n",
       " 'sk': 524,\n",
       " 'sl': 525,\n",
       " 'sm': 526,\n",
       " 'sn': 527,\n",
       " 'so': 528,\n",
       " 'sp': 529,\n",
       " 'sq': 530,\n",
       " 'sr': 531,\n",
       " 'ss': 532,\n",
       " 'st': 533,\n",
       " 'su': 534,\n",
       " 'sv': 535,\n",
       " 'sw': 536,\n",
       " 'sx': 537,\n",
       " 'sy': 538,\n",
       " 'sz': 539,\n",
       " 't.': 540,\n",
       " 'ta': 541,\n",
       " 'tb': 542,\n",
       " 'tc': 543,\n",
       " 'td': 544,\n",
       " 'te': 545,\n",
       " 'tf': 546,\n",
       " 'tg': 547,\n",
       " 'th': 548,\n",
       " 'ti': 549,\n",
       " 'tj': 550,\n",
       " 'tk': 551,\n",
       " 'tl': 552,\n",
       " 'tm': 553,\n",
       " 'tn': 554,\n",
       " 'to': 555,\n",
       " 'tp': 556,\n",
       " 'tq': 557,\n",
       " 'tr': 558,\n",
       " 'ts': 559,\n",
       " 'tt': 560,\n",
       " 'tu': 561,\n",
       " 'tv': 562,\n",
       " 'tw': 563,\n",
       " 'tx': 564,\n",
       " 'ty': 565,\n",
       " 'tz': 566,\n",
       " 'u.': 567,\n",
       " 'ua': 568,\n",
       " 'ub': 569,\n",
       " 'uc': 570,\n",
       " 'ud': 571,\n",
       " 'ue': 572,\n",
       " 'uf': 573,\n",
       " 'ug': 574,\n",
       " 'uh': 575,\n",
       " 'ui': 576,\n",
       " 'uj': 577,\n",
       " 'uk': 578,\n",
       " 'ul': 579,\n",
       " 'um': 580,\n",
       " 'un': 581,\n",
       " 'uo': 582,\n",
       " 'up': 583,\n",
       " 'uq': 584,\n",
       " 'ur': 585,\n",
       " 'us': 586,\n",
       " 'ut': 587,\n",
       " 'uu': 588,\n",
       " 'uv': 589,\n",
       " 'uw': 590,\n",
       " 'ux': 591,\n",
       " 'uy': 592,\n",
       " 'uz': 593,\n",
       " 'v.': 594,\n",
       " 'va': 595,\n",
       " 'vb': 596,\n",
       " 'vc': 597,\n",
       " 'vd': 598,\n",
       " 've': 599,\n",
       " 'vf': 600,\n",
       " 'vg': 601,\n",
       " 'vh': 602,\n",
       " 'vi': 603,\n",
       " 'vj': 604,\n",
       " 'vk': 605,\n",
       " 'vl': 606,\n",
       " 'vm': 607,\n",
       " 'vn': 608,\n",
       " 'vo': 609,\n",
       " 'vp': 610,\n",
       " 'vq': 611,\n",
       " 'vr': 612,\n",
       " 'vs': 613,\n",
       " 'vt': 614,\n",
       " 'vu': 615,\n",
       " 'vv': 616,\n",
       " 'vw': 617,\n",
       " 'vx': 618,\n",
       " 'vy': 619,\n",
       " 'vz': 620,\n",
       " 'w.': 621,\n",
       " 'wa': 622,\n",
       " 'wb': 623,\n",
       " 'wc': 624,\n",
       " 'wd': 625,\n",
       " 'we': 626,\n",
       " 'wf': 627,\n",
       " 'wg': 628,\n",
       " 'wh': 629,\n",
       " 'wi': 630,\n",
       " 'wj': 631,\n",
       " 'wk': 632,\n",
       " 'wl': 633,\n",
       " 'wm': 634,\n",
       " 'wn': 635,\n",
       " 'wo': 636,\n",
       " 'wp': 637,\n",
       " 'wq': 638,\n",
       " 'wr': 639,\n",
       " 'ws': 640,\n",
       " 'wt': 641,\n",
       " 'wu': 642,\n",
       " 'wv': 643,\n",
       " 'ww': 644,\n",
       " 'wx': 645,\n",
       " 'wy': 646,\n",
       " 'wz': 647,\n",
       " 'x.': 648,\n",
       " 'xa': 649,\n",
       " 'xb': 650,\n",
       " 'xc': 651,\n",
       " 'xd': 652,\n",
       " 'xe': 653,\n",
       " 'xf': 654,\n",
       " 'xg': 655,\n",
       " 'xh': 656,\n",
       " 'xi': 657,\n",
       " 'xj': 658,\n",
       " 'xk': 659,\n",
       " 'xl': 660,\n",
       " 'xm': 661,\n",
       " 'xn': 662,\n",
       " 'xo': 663,\n",
       " 'xp': 664,\n",
       " 'xq': 665,\n",
       " 'xr': 666,\n",
       " 'xs': 667,\n",
       " 'xt': 668,\n",
       " 'xu': 669,\n",
       " 'xv': 670,\n",
       " 'xw': 671,\n",
       " 'xx': 672,\n",
       " 'xy': 673,\n",
       " 'xz': 674,\n",
       " 'y.': 675,\n",
       " 'ya': 676,\n",
       " 'yb': 677,\n",
       " 'yc': 678,\n",
       " 'yd': 679,\n",
       " 'ye': 680,\n",
       " 'yf': 681,\n",
       " 'yg': 682,\n",
       " 'yh': 683,\n",
       " 'yi': 684,\n",
       " 'yj': 685,\n",
       " 'yk': 686,\n",
       " 'yl': 687,\n",
       " 'ym': 688,\n",
       " 'yn': 689,\n",
       " 'yo': 690,\n",
       " 'yp': 691,\n",
       " 'yq': 692,\n",
       " 'yr': 693,\n",
       " 'ys': 694,\n",
       " 'yt': 695,\n",
       " 'yu': 696,\n",
       " 'yv': 697,\n",
       " 'yw': 698,\n",
       " 'yx': 699,\n",
       " 'yy': 700,\n",
       " 'yz': 701,\n",
       " 'z.': 702,\n",
       " 'za': 703,\n",
       " 'zb': 704,\n",
       " 'zc': 705,\n",
       " 'zd': 706,\n",
       " 'ze': 707,\n",
       " 'zf': 708,\n",
       " 'zg': 709,\n",
       " 'zh': 710,\n",
       " 'zi': 711,\n",
       " 'zj': 712,\n",
       " 'zk': 713,\n",
       " 'zl': 714,\n",
       " 'zm': 715,\n",
       " 'zn': 716,\n",
       " 'zo': 717,\n",
       " 'zp': 718,\n",
       " 'zq': 719,\n",
       " 'zr': 720,\n",
       " 'zs': 721,\n",
       " 'zt': 722,\n",
       " 'zu': 723,\n",
       " 'zv': 724,\n",
       " 'zw': 725,\n",
       " 'zx': 726,\n",
       " 'zy': 727,\n",
       " 'zz': 728}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f2c1647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(729, 27)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create integer to string dictionary\n",
    "\n",
    "itos ={i:s for s,i in stoi.items()}\n",
    "\n",
    "itos2 = {i:s for s, i in stoi2.items()}\n",
    "len(itos), len(itos2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a2dac34a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '..',\n",
       " 1: '.a',\n",
       " 2: '.b',\n",
       " 3: '.c',\n",
       " 4: '.d',\n",
       " 5: '.e',\n",
       " 6: '.f',\n",
       " 7: '.g',\n",
       " 8: '.h',\n",
       " 9: '.i',\n",
       " 10: '.j',\n",
       " 11: '.k',\n",
       " 12: '.l',\n",
       " 13: '.m',\n",
       " 14: '.n',\n",
       " 15: '.o',\n",
       " 16: '.p',\n",
       " 17: '.q',\n",
       " 18: '.r',\n",
       " 19: '.s',\n",
       " 20: '.t',\n",
       " 21: '.u',\n",
       " 22: '.v',\n",
       " 23: '.w',\n",
       " 24: '.x',\n",
       " 25: '.y',\n",
       " 26: '.z',\n",
       " 27: 'a.',\n",
       " 28: 'aa',\n",
       " 29: 'ab',\n",
       " 30: 'ac',\n",
       " 31: 'ad',\n",
       " 32: 'ae',\n",
       " 33: 'af',\n",
       " 34: 'ag',\n",
       " 35: 'ah',\n",
       " 36: 'ai',\n",
       " 37: 'aj',\n",
       " 38: 'ak',\n",
       " 39: 'al',\n",
       " 40: 'am',\n",
       " 41: 'an',\n",
       " 42: 'ao',\n",
       " 43: 'ap',\n",
       " 44: 'aq',\n",
       " 45: 'ar',\n",
       " 46: 'as',\n",
       " 47: 'at',\n",
       " 48: 'au',\n",
       " 49: 'av',\n",
       " 50: 'aw',\n",
       " 51: 'ax',\n",
       " 52: 'ay',\n",
       " 53: 'az',\n",
       " 54: 'b.',\n",
       " 55: 'ba',\n",
       " 56: 'bb',\n",
       " 57: 'bc',\n",
       " 58: 'bd',\n",
       " 59: 'be',\n",
       " 60: 'bf',\n",
       " 61: 'bg',\n",
       " 62: 'bh',\n",
       " 63: 'bi',\n",
       " 64: 'bj',\n",
       " 65: 'bk',\n",
       " 66: 'bl',\n",
       " 67: 'bm',\n",
       " 68: 'bn',\n",
       " 69: 'bo',\n",
       " 70: 'bp',\n",
       " 71: 'bq',\n",
       " 72: 'br',\n",
       " 73: 'bs',\n",
       " 74: 'bt',\n",
       " 75: 'bu',\n",
       " 76: 'bv',\n",
       " 77: 'bw',\n",
       " 78: 'bx',\n",
       " 79: 'by',\n",
       " 80: 'bz',\n",
       " 81: 'c.',\n",
       " 82: 'ca',\n",
       " 83: 'cb',\n",
       " 84: 'cc',\n",
       " 85: 'cd',\n",
       " 86: 'ce',\n",
       " 87: 'cf',\n",
       " 88: 'cg',\n",
       " 89: 'ch',\n",
       " 90: 'ci',\n",
       " 91: 'cj',\n",
       " 92: 'ck',\n",
       " 93: 'cl',\n",
       " 94: 'cm',\n",
       " 95: 'cn',\n",
       " 96: 'co',\n",
       " 97: 'cp',\n",
       " 98: 'cq',\n",
       " 99: 'cr',\n",
       " 100: 'cs',\n",
       " 101: 'ct',\n",
       " 102: 'cu',\n",
       " 103: 'cv',\n",
       " 104: 'cw',\n",
       " 105: 'cx',\n",
       " 106: 'cy',\n",
       " 107: 'cz',\n",
       " 108: 'd.',\n",
       " 109: 'da',\n",
       " 110: 'db',\n",
       " 111: 'dc',\n",
       " 112: 'dd',\n",
       " 113: 'de',\n",
       " 114: 'df',\n",
       " 115: 'dg',\n",
       " 116: 'dh',\n",
       " 117: 'di',\n",
       " 118: 'dj',\n",
       " 119: 'dk',\n",
       " 120: 'dl',\n",
       " 121: 'dm',\n",
       " 122: 'dn',\n",
       " 123: 'do',\n",
       " 124: 'dp',\n",
       " 125: 'dq',\n",
       " 126: 'dr',\n",
       " 127: 'ds',\n",
       " 128: 'dt',\n",
       " 129: 'du',\n",
       " 130: 'dv',\n",
       " 131: 'dw',\n",
       " 132: 'dx',\n",
       " 133: 'dy',\n",
       " 134: 'dz',\n",
       " 135: 'e.',\n",
       " 136: 'ea',\n",
       " 137: 'eb',\n",
       " 138: 'ec',\n",
       " 139: 'ed',\n",
       " 140: 'ee',\n",
       " 141: 'ef',\n",
       " 142: 'eg',\n",
       " 143: 'eh',\n",
       " 144: 'ei',\n",
       " 145: 'ej',\n",
       " 146: 'ek',\n",
       " 147: 'el',\n",
       " 148: 'em',\n",
       " 149: 'en',\n",
       " 150: 'eo',\n",
       " 151: 'ep',\n",
       " 152: 'eq',\n",
       " 153: 'er',\n",
       " 154: 'es',\n",
       " 155: 'et',\n",
       " 156: 'eu',\n",
       " 157: 'ev',\n",
       " 158: 'ew',\n",
       " 159: 'ex',\n",
       " 160: 'ey',\n",
       " 161: 'ez',\n",
       " 162: 'f.',\n",
       " 163: 'fa',\n",
       " 164: 'fb',\n",
       " 165: 'fc',\n",
       " 166: 'fd',\n",
       " 167: 'fe',\n",
       " 168: 'ff',\n",
       " 169: 'fg',\n",
       " 170: 'fh',\n",
       " 171: 'fi',\n",
       " 172: 'fj',\n",
       " 173: 'fk',\n",
       " 174: 'fl',\n",
       " 175: 'fm',\n",
       " 176: 'fn',\n",
       " 177: 'fo',\n",
       " 178: 'fp',\n",
       " 179: 'fq',\n",
       " 180: 'fr',\n",
       " 181: 'fs',\n",
       " 182: 'ft',\n",
       " 183: 'fu',\n",
       " 184: 'fv',\n",
       " 185: 'fw',\n",
       " 186: 'fx',\n",
       " 187: 'fy',\n",
       " 188: 'fz',\n",
       " 189: 'g.',\n",
       " 190: 'ga',\n",
       " 191: 'gb',\n",
       " 192: 'gc',\n",
       " 193: 'gd',\n",
       " 194: 'ge',\n",
       " 195: 'gf',\n",
       " 196: 'gg',\n",
       " 197: 'gh',\n",
       " 198: 'gi',\n",
       " 199: 'gj',\n",
       " 200: 'gk',\n",
       " 201: 'gl',\n",
       " 202: 'gm',\n",
       " 203: 'gn',\n",
       " 204: 'go',\n",
       " 205: 'gp',\n",
       " 206: 'gq',\n",
       " 207: 'gr',\n",
       " 208: 'gs',\n",
       " 209: 'gt',\n",
       " 210: 'gu',\n",
       " 211: 'gv',\n",
       " 212: 'gw',\n",
       " 213: 'gx',\n",
       " 214: 'gy',\n",
       " 215: 'gz',\n",
       " 216: 'h.',\n",
       " 217: 'ha',\n",
       " 218: 'hb',\n",
       " 219: 'hc',\n",
       " 220: 'hd',\n",
       " 221: 'he',\n",
       " 222: 'hf',\n",
       " 223: 'hg',\n",
       " 224: 'hh',\n",
       " 225: 'hi',\n",
       " 226: 'hj',\n",
       " 227: 'hk',\n",
       " 228: 'hl',\n",
       " 229: 'hm',\n",
       " 230: 'hn',\n",
       " 231: 'ho',\n",
       " 232: 'hp',\n",
       " 233: 'hq',\n",
       " 234: 'hr',\n",
       " 235: 'hs',\n",
       " 236: 'ht',\n",
       " 237: 'hu',\n",
       " 238: 'hv',\n",
       " 239: 'hw',\n",
       " 240: 'hx',\n",
       " 241: 'hy',\n",
       " 242: 'hz',\n",
       " 243: 'i.',\n",
       " 244: 'ia',\n",
       " 245: 'ib',\n",
       " 246: 'ic',\n",
       " 247: 'id',\n",
       " 248: 'ie',\n",
       " 249: 'if',\n",
       " 250: 'ig',\n",
       " 251: 'ih',\n",
       " 252: 'ii',\n",
       " 253: 'ij',\n",
       " 254: 'ik',\n",
       " 255: 'il',\n",
       " 256: 'im',\n",
       " 257: 'in',\n",
       " 258: 'io',\n",
       " 259: 'ip',\n",
       " 260: 'iq',\n",
       " 261: 'ir',\n",
       " 262: 'is',\n",
       " 263: 'it',\n",
       " 264: 'iu',\n",
       " 265: 'iv',\n",
       " 266: 'iw',\n",
       " 267: 'ix',\n",
       " 268: 'iy',\n",
       " 269: 'iz',\n",
       " 270: 'j.',\n",
       " 271: 'ja',\n",
       " 272: 'jb',\n",
       " 273: 'jc',\n",
       " 274: 'jd',\n",
       " 275: 'je',\n",
       " 276: 'jf',\n",
       " 277: 'jg',\n",
       " 278: 'jh',\n",
       " 279: 'ji',\n",
       " 280: 'jj',\n",
       " 281: 'jk',\n",
       " 282: 'jl',\n",
       " 283: 'jm',\n",
       " 284: 'jn',\n",
       " 285: 'jo',\n",
       " 286: 'jp',\n",
       " 287: 'jq',\n",
       " 288: 'jr',\n",
       " 289: 'js',\n",
       " 290: 'jt',\n",
       " 291: 'ju',\n",
       " 292: 'jv',\n",
       " 293: 'jw',\n",
       " 294: 'jx',\n",
       " 295: 'jy',\n",
       " 296: 'jz',\n",
       " 297: 'k.',\n",
       " 298: 'ka',\n",
       " 299: 'kb',\n",
       " 300: 'kc',\n",
       " 301: 'kd',\n",
       " 302: 'ke',\n",
       " 303: 'kf',\n",
       " 304: 'kg',\n",
       " 305: 'kh',\n",
       " 306: 'ki',\n",
       " 307: 'kj',\n",
       " 308: 'kk',\n",
       " 309: 'kl',\n",
       " 310: 'km',\n",
       " 311: 'kn',\n",
       " 312: 'ko',\n",
       " 313: 'kp',\n",
       " 314: 'kq',\n",
       " 315: 'kr',\n",
       " 316: 'ks',\n",
       " 317: 'kt',\n",
       " 318: 'ku',\n",
       " 319: 'kv',\n",
       " 320: 'kw',\n",
       " 321: 'kx',\n",
       " 322: 'ky',\n",
       " 323: 'kz',\n",
       " 324: 'l.',\n",
       " 325: 'la',\n",
       " 326: 'lb',\n",
       " 327: 'lc',\n",
       " 328: 'ld',\n",
       " 329: 'le',\n",
       " 330: 'lf',\n",
       " 331: 'lg',\n",
       " 332: 'lh',\n",
       " 333: 'li',\n",
       " 334: 'lj',\n",
       " 335: 'lk',\n",
       " 336: 'll',\n",
       " 337: 'lm',\n",
       " 338: 'ln',\n",
       " 339: 'lo',\n",
       " 340: 'lp',\n",
       " 341: 'lq',\n",
       " 342: 'lr',\n",
       " 343: 'ls',\n",
       " 344: 'lt',\n",
       " 345: 'lu',\n",
       " 346: 'lv',\n",
       " 347: 'lw',\n",
       " 348: 'lx',\n",
       " 349: 'ly',\n",
       " 350: 'lz',\n",
       " 351: 'm.',\n",
       " 352: 'ma',\n",
       " 353: 'mb',\n",
       " 354: 'mc',\n",
       " 355: 'md',\n",
       " 356: 'me',\n",
       " 357: 'mf',\n",
       " 358: 'mg',\n",
       " 359: 'mh',\n",
       " 360: 'mi',\n",
       " 361: 'mj',\n",
       " 362: 'mk',\n",
       " 363: 'ml',\n",
       " 364: 'mm',\n",
       " 365: 'mn',\n",
       " 366: 'mo',\n",
       " 367: 'mp',\n",
       " 368: 'mq',\n",
       " 369: 'mr',\n",
       " 370: 'ms',\n",
       " 371: 'mt',\n",
       " 372: 'mu',\n",
       " 373: 'mv',\n",
       " 374: 'mw',\n",
       " 375: 'mx',\n",
       " 376: 'my',\n",
       " 377: 'mz',\n",
       " 378: 'n.',\n",
       " 379: 'na',\n",
       " 380: 'nb',\n",
       " 381: 'nc',\n",
       " 382: 'nd',\n",
       " 383: 'ne',\n",
       " 384: 'nf',\n",
       " 385: 'ng',\n",
       " 386: 'nh',\n",
       " 387: 'ni',\n",
       " 388: 'nj',\n",
       " 389: 'nk',\n",
       " 390: 'nl',\n",
       " 391: 'nm',\n",
       " 392: 'nn',\n",
       " 393: 'no',\n",
       " 394: 'np',\n",
       " 395: 'nq',\n",
       " 396: 'nr',\n",
       " 397: 'ns',\n",
       " 398: 'nt',\n",
       " 399: 'nu',\n",
       " 400: 'nv',\n",
       " 401: 'nw',\n",
       " 402: 'nx',\n",
       " 403: 'ny',\n",
       " 404: 'nz',\n",
       " 405: 'o.',\n",
       " 406: 'oa',\n",
       " 407: 'ob',\n",
       " 408: 'oc',\n",
       " 409: 'od',\n",
       " 410: 'oe',\n",
       " 411: 'of',\n",
       " 412: 'og',\n",
       " 413: 'oh',\n",
       " 414: 'oi',\n",
       " 415: 'oj',\n",
       " 416: 'ok',\n",
       " 417: 'ol',\n",
       " 418: 'om',\n",
       " 419: 'on',\n",
       " 420: 'oo',\n",
       " 421: 'op',\n",
       " 422: 'oq',\n",
       " 423: 'or',\n",
       " 424: 'os',\n",
       " 425: 'ot',\n",
       " 426: 'ou',\n",
       " 427: 'ov',\n",
       " 428: 'ow',\n",
       " 429: 'ox',\n",
       " 430: 'oy',\n",
       " 431: 'oz',\n",
       " 432: 'p.',\n",
       " 433: 'pa',\n",
       " 434: 'pb',\n",
       " 435: 'pc',\n",
       " 436: 'pd',\n",
       " 437: 'pe',\n",
       " 438: 'pf',\n",
       " 439: 'pg',\n",
       " 440: 'ph',\n",
       " 441: 'pi',\n",
       " 442: 'pj',\n",
       " 443: 'pk',\n",
       " 444: 'pl',\n",
       " 445: 'pm',\n",
       " 446: 'pn',\n",
       " 447: 'po',\n",
       " 448: 'pp',\n",
       " 449: 'pq',\n",
       " 450: 'pr',\n",
       " 451: 'ps',\n",
       " 452: 'pt',\n",
       " 453: 'pu',\n",
       " 454: 'pv',\n",
       " 455: 'pw',\n",
       " 456: 'px',\n",
       " 457: 'py',\n",
       " 458: 'pz',\n",
       " 459: 'q.',\n",
       " 460: 'qa',\n",
       " 461: 'qb',\n",
       " 462: 'qc',\n",
       " 463: 'qd',\n",
       " 464: 'qe',\n",
       " 465: 'qf',\n",
       " 466: 'qg',\n",
       " 467: 'qh',\n",
       " 468: 'qi',\n",
       " 469: 'qj',\n",
       " 470: 'qk',\n",
       " 471: 'ql',\n",
       " 472: 'qm',\n",
       " 473: 'qn',\n",
       " 474: 'qo',\n",
       " 475: 'qp',\n",
       " 476: 'qq',\n",
       " 477: 'qr',\n",
       " 478: 'qs',\n",
       " 479: 'qt',\n",
       " 480: 'qu',\n",
       " 481: 'qv',\n",
       " 482: 'qw',\n",
       " 483: 'qx',\n",
       " 484: 'qy',\n",
       " 485: 'qz',\n",
       " 486: 'r.',\n",
       " 487: 'ra',\n",
       " 488: 'rb',\n",
       " 489: 'rc',\n",
       " 490: 'rd',\n",
       " 491: 're',\n",
       " 492: 'rf',\n",
       " 493: 'rg',\n",
       " 494: 'rh',\n",
       " 495: 'ri',\n",
       " 496: 'rj',\n",
       " 497: 'rk',\n",
       " 498: 'rl',\n",
       " 499: 'rm',\n",
       " 500: 'rn',\n",
       " 501: 'ro',\n",
       " 502: 'rp',\n",
       " 503: 'rq',\n",
       " 504: 'rr',\n",
       " 505: 'rs',\n",
       " 506: 'rt',\n",
       " 507: 'ru',\n",
       " 508: 'rv',\n",
       " 509: 'rw',\n",
       " 510: 'rx',\n",
       " 511: 'ry',\n",
       " 512: 'rz',\n",
       " 513: 's.',\n",
       " 514: 'sa',\n",
       " 515: 'sb',\n",
       " 516: 'sc',\n",
       " 517: 'sd',\n",
       " 518: 'se',\n",
       " 519: 'sf',\n",
       " 520: 'sg',\n",
       " 521: 'sh',\n",
       " 522: 'si',\n",
       " 523: 'sj',\n",
       " 524: 'sk',\n",
       " 525: 'sl',\n",
       " 526: 'sm',\n",
       " 527: 'sn',\n",
       " 528: 'so',\n",
       " 529: 'sp',\n",
       " 530: 'sq',\n",
       " 531: 'sr',\n",
       " 532: 'ss',\n",
       " 533: 'st',\n",
       " 534: 'su',\n",
       " 535: 'sv',\n",
       " 536: 'sw',\n",
       " 537: 'sx',\n",
       " 538: 'sy',\n",
       " 539: 'sz',\n",
       " 540: 't.',\n",
       " 541: 'ta',\n",
       " 542: 'tb',\n",
       " 543: 'tc',\n",
       " 544: 'td',\n",
       " 545: 'te',\n",
       " 546: 'tf',\n",
       " 547: 'tg',\n",
       " 548: 'th',\n",
       " 549: 'ti',\n",
       " 550: 'tj',\n",
       " 551: 'tk',\n",
       " 552: 'tl',\n",
       " 553: 'tm',\n",
       " 554: 'tn',\n",
       " 555: 'to',\n",
       " 556: 'tp',\n",
       " 557: 'tq',\n",
       " 558: 'tr',\n",
       " 559: 'ts',\n",
       " 560: 'tt',\n",
       " 561: 'tu',\n",
       " 562: 'tv',\n",
       " 563: 'tw',\n",
       " 564: 'tx',\n",
       " 565: 'ty',\n",
       " 566: 'tz',\n",
       " 567: 'u.',\n",
       " 568: 'ua',\n",
       " 569: 'ub',\n",
       " 570: 'uc',\n",
       " 571: 'ud',\n",
       " 572: 'ue',\n",
       " 573: 'uf',\n",
       " 574: 'ug',\n",
       " 575: 'uh',\n",
       " 576: 'ui',\n",
       " 577: 'uj',\n",
       " 578: 'uk',\n",
       " 579: 'ul',\n",
       " 580: 'um',\n",
       " 581: 'un',\n",
       " 582: 'uo',\n",
       " 583: 'up',\n",
       " 584: 'uq',\n",
       " 585: 'ur',\n",
       " 586: 'us',\n",
       " 587: 'ut',\n",
       " 588: 'uu',\n",
       " 589: 'uv',\n",
       " 590: 'uw',\n",
       " 591: 'ux',\n",
       " 592: 'uy',\n",
       " 593: 'uz',\n",
       " 594: 'v.',\n",
       " 595: 'va',\n",
       " 596: 'vb',\n",
       " 597: 'vc',\n",
       " 598: 'vd',\n",
       " 599: 've',\n",
       " 600: 'vf',\n",
       " 601: 'vg',\n",
       " 602: 'vh',\n",
       " 603: 'vi',\n",
       " 604: 'vj',\n",
       " 605: 'vk',\n",
       " 606: 'vl',\n",
       " 607: 'vm',\n",
       " 608: 'vn',\n",
       " 609: 'vo',\n",
       " 610: 'vp',\n",
       " 611: 'vq',\n",
       " 612: 'vr',\n",
       " 613: 'vs',\n",
       " 614: 'vt',\n",
       " 615: 'vu',\n",
       " 616: 'vv',\n",
       " 617: 'vw',\n",
       " 618: 'vx',\n",
       " 619: 'vy',\n",
       " 620: 'vz',\n",
       " 621: 'w.',\n",
       " 622: 'wa',\n",
       " 623: 'wb',\n",
       " 624: 'wc',\n",
       " 625: 'wd',\n",
       " 626: 'we',\n",
       " 627: 'wf',\n",
       " 628: 'wg',\n",
       " 629: 'wh',\n",
       " 630: 'wi',\n",
       " 631: 'wj',\n",
       " 632: 'wk',\n",
       " 633: 'wl',\n",
       " 634: 'wm',\n",
       " 635: 'wn',\n",
       " 636: 'wo',\n",
       " 637: 'wp',\n",
       " 638: 'wq',\n",
       " 639: 'wr',\n",
       " 640: 'ws',\n",
       " 641: 'wt',\n",
       " 642: 'wu',\n",
       " 643: 'wv',\n",
       " 644: 'ww',\n",
       " 645: 'wx',\n",
       " 646: 'wy',\n",
       " 647: 'wz',\n",
       " 648: 'x.',\n",
       " 649: 'xa',\n",
       " 650: 'xb',\n",
       " 651: 'xc',\n",
       " 652: 'xd',\n",
       " 653: 'xe',\n",
       " 654: 'xf',\n",
       " 655: 'xg',\n",
       " 656: 'xh',\n",
       " 657: 'xi',\n",
       " 658: 'xj',\n",
       " 659: 'xk',\n",
       " 660: 'xl',\n",
       " 661: 'xm',\n",
       " 662: 'xn',\n",
       " 663: 'xo',\n",
       " 664: 'xp',\n",
       " 665: 'xq',\n",
       " 666: 'xr',\n",
       " 667: 'xs',\n",
       " 668: 'xt',\n",
       " 669: 'xu',\n",
       " 670: 'xv',\n",
       " 671: 'xw',\n",
       " 672: 'xx',\n",
       " 673: 'xy',\n",
       " 674: 'xz',\n",
       " 675: 'y.',\n",
       " 676: 'ya',\n",
       " 677: 'yb',\n",
       " 678: 'yc',\n",
       " 679: 'yd',\n",
       " 680: 'ye',\n",
       " 681: 'yf',\n",
       " 682: 'yg',\n",
       " 683: 'yh',\n",
       " 684: 'yi',\n",
       " 685: 'yj',\n",
       " 686: 'yk',\n",
       " 687: 'yl',\n",
       " 688: 'ym',\n",
       " 689: 'yn',\n",
       " 690: 'yo',\n",
       " 691: 'yp',\n",
       " 692: 'yq',\n",
       " 693: 'yr',\n",
       " 694: 'ys',\n",
       " 695: 'yt',\n",
       " 696: 'yu',\n",
       " 697: 'yv',\n",
       " 698: 'yw',\n",
       " 699: 'yx',\n",
       " 700: 'yy',\n",
       " 701: 'yz',\n",
       " 702: 'z.',\n",
       " 703: 'za',\n",
       " 704: 'zb',\n",
       " 705: 'zc',\n",
       " 706: 'zd',\n",
       " 707: 'ze',\n",
       " 708: 'zf',\n",
       " 709: 'zg',\n",
       " 710: 'zh',\n",
       " 711: 'zi',\n",
       " 712: 'zj',\n",
       " 713: 'zk',\n",
       " 714: 'zl',\n",
       " 715: 'zm',\n",
       " 716: 'zn',\n",
       " 717: 'zo',\n",
       " 718: 'zp',\n",
       " 719: 'zq',\n",
       " 720: 'zr',\n",
       " 721: 'zs',\n",
       " 722: 'zt',\n",
       " 723: 'zu',\n",
       " 724: 'zv',\n",
       " 725: 'zw',\n",
       " 726: 'zx',\n",
       " 727: 'zy',\n",
       " 728: 'zz'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4cf3b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2c09342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".e m\n",
      "em m\n",
      "mm a\n",
      "ma .\n",
      ".o l\n",
      "ol i\n",
      "li v\n",
      "iv i\n",
      "vi a\n",
      "ia .\n",
      ".a v\n",
      "av a\n",
      "va .\n"
     ]
    }
   ],
   "source": [
    "# Extract training data from our word list for the trigram model\n",
    "xs = []\n",
    "ys = []\n",
    "for w in words[:3]:\n",
    "    chs = ['.'] + list(w) +['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ch = ch1+ch2\n",
    "        ix1 = stoi[ch]\n",
    "        ix2 = stoi2[ch3]\n",
    "        print(ch,ch3)\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "         \n",
    "# converting to tensor\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83aa7614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([706, 120, 336, 324, 716, 389, 305, 237, 575, 216, 702,  21, 567]),\n",
       " tensor([12, 12,  0, 26, 11,  8, 21,  8,  0, 26, 21,  0, 26]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "524e5a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of sending one character value to the neuron, we will get one hot encoding for all 729 combinations\n",
    "\n",
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(xs, num_classes = 729).float()   # one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3abad95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 729])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67501c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 27])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For 729 possible characters, there are 27 possible output features\n",
    "\n",
    "W = torch.randn((729,27))\n",
    "\n",
    "(xenc @ W).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3165c75d",
   "metadata": {},
   "source": [
    "## Trigram model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11190ca",
   "metadata": {},
   "source": [
    "### Creating the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "977419f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  196113\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ch = ch1+ch2\n",
    "        ix1 = stoi[ch]\n",
    "        ix2 = stoi2[ch3]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "        \n",
    "# converting to tensor        \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()  # total elements/items\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((729, 27), generator=g, requires_grad=True)   # Gradient is set to true, to perform backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e4bd21",
   "metadata": {},
   "source": [
    "### Training the trigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d0136121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward propagation - gradient descent\n",
    "for k in range(100):  # 1000 epochs\n",
    "  \n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=729).float() # input to the network: one-hot encoding\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean() # regularization of loss\n",
    "    #print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None # set to zero the gradient\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "81415856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proaehqz.\n",
      "memilisesnatomadaaacozavisabqlisypadajuemaevmazamamemaxwusautaravaralareliymadelithamanabfaevaladanpromassaanaauchensizasazhalaylqaizwpcusalemaledediqharauyuzhaypcnivkeijekarivu.\n",
      "kat.\n"
     ]
    }
   ],
   "source": [
    "# sample from the 'neural net' model - Trigram\n",
    "g = torch.Generator().manual_seed(10)\n",
    "\n",
    "for i in range(3):      # no. of words to predict\n",
    "  \n",
    "  out = []\n",
    "  ix = 1\n",
    "  while True:\n",
    "    xenc = F.one_hot(torch.tensor([ix]), num_classes=729).float()\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    # ----------\n",
    "    \n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    out.append(itos2[ix])\n",
    "    if ix == 0:\n",
    "      break\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899558c9",
   "metadata": {},
   "source": [
    "### Trigram model with counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a8ad1cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((729, 27), dtype=torch.int32) # 27*27 input features and 27 output features \n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) +['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ch = ch1+ch2\n",
    "        ix1 = stoi[ch]\n",
    "        ix2 = stoi2[ch3]\n",
    "        \n",
    "        N[ix1, ix2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "93bd5791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyp.\n",
      "e.\n",
      "qz.\n",
      "e.\n",
      "ybyi.\n",
      "nanatomadianariri.\n",
      "sanyanaypyaniue.\n",
      "wcimazmadiemahwurali.\n",
      "odiararare.\n",
      "iymyhorithke.\n"
     ]
    }
   ],
   "source": [
    "# To get probability matrix with respect to rows\n",
    "P = (N+1).float()                      # add 1 as a fake count, so that none of the combinations of bigrams, gives -- \n",
    "                                       # -- 0 count which lead to the value of infinity(log likelihood)\n",
    "# Broadcasting semantics\n",
    "P = P / P.sum(1, keepdim=True)         # Broadcasting\n",
    "\n",
    "\n",
    "g = torch.Generator().manual_seed(10)\n",
    "\n",
    "for i in range(10):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        p=P[ix]\n",
    "        \n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        #print(itos[ix])\n",
    "        out.append(itos2[ix])\n",
    "        ix = stoi[itos[ix][1]+itos2[ix]]\n",
    "        if ix == 0:\n",
    "            break\n",
    "    #print(out)\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4133bd8a",
   "metadata": {},
   "source": [
    "### Loss Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0ba8f169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".e : 0.1855\n",
      "em : 0.1269\n",
      "mm : 0.3744\n",
      "ma : 0.0669\n",
      ".o : 0.2494\n",
      "ol : 0.1084\n",
      "li : 0.0219\n",
      "iv : 0.2669\n",
      "vi : 0.1578\n",
      "ia : 0.3657\n",
      ".a : 0.0550\n",
      "av : 0.1882\n",
      "va : 0.1405\n",
      "ll=tensor(-25.5742), avgll=tensor(-1.9672), nll=tensor(1.9672)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ll=0.0 # Initiating log likelihood\n",
    "n=0\n",
    "\n",
    "for w in words[:3]:\n",
    "    chs = ['.'] + list(w) +['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ch = ch1+ch2\n",
    "        ix1 = stoi[ch]\n",
    "        ix2 = stoi2[ch3]\n",
    "        prob = P[ix1,ix2]\n",
    "        print(f'{ch1}{ch2} : {prob:.4f}')\n",
    "\n",
    "        # For a good estimation of words, the probability of our selected bigram in our model should be closer to 1\n",
    "        # if probability is closer to 1 then its log should be closer to 0 ---> log 1 = 0\n",
    "        # log of values closer to 0, gives - inf  -----> log 0 = -inf\n",
    "        # to get a single value for the quality check estimation, we do product of probability of all values\n",
    "        # which is equal to the sum of logs (log of likelihood), log(p1*p2*p3) = log p1 + log p2 + log p3\n",
    "        # since every value of probability is between 0 and 1, we get log to be a negative value\n",
    "        # Instead of maximizing the negative values, we are going to minimize -log\n",
    "        \n",
    "        ll += torch.log(prob)\n",
    "        n+=1\n",
    "\n",
    "avgll = ll/n # average of log likelihood\n",
    "nll = -avgll \n",
    "print(f'{ll=}, {avgll=}, {nll=}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce65e7e",
   "metadata": {},
   "source": [
    "### Comments:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28850531",
   "metadata": {},
   "source": [
    "#### Trigram model has reduced loss compared compared to bigram model .\n",
    "\n",
    "loss of trigram model = 1.972, loss of bigram model = 2.4241 [Based on counting]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5bdcff",
   "metadata": {},
   "source": [
    "### Splitting the input and output features into 80% train, 10% dev, 10% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbdc7158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32033, 25626, 3203, 3204)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(words) # total number of words\n",
    "\n",
    "# Calculate the split indices\n",
    "train_size = int(0.8 * n)\n",
    "val_size = int(0.1 * n)\n",
    "test_size = n - train_size - val_size\n",
    "\n",
    "# Create the splits\n",
    "w_train = words[:train_size]\n",
    "w_val = words[train_size:train_size + val_size]\n",
    "w_test = words[train_size + val_size:]\n",
    "\n",
    "n, len(w_train), len(w_val), len(w_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5eb88c0",
   "metadata": {},
   "source": [
    "#### loss of validation sets and test sets, when probability matrix is created for the full set of words based on counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e0f06f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ll=tensor(-43392.3047), avgll=tensor(-2.2333), nll=tensor(2.2333)\n"
     ]
    }
   ],
   "source": [
    "ll=0.0 # Initiating log likelihood\n",
    "n=0\n",
    "\n",
    "for w in w_val:\n",
    "    chs = ['.'] + list(w) +['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ch = ch1+ch2\n",
    "        ix1 = stoi[ch]\n",
    "        ix2 = stoi2[ch3]\n",
    "        prob = P[ix1,ix2]\n",
    "        \n",
    "        ll += torch.log(prob)\n",
    "        n+=1\n",
    "\n",
    "avgll = ll/n # average of log likelihood\n",
    "nll = -avgll \n",
    "print(f'{ll=}, {avgll=}, {nll=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "51f65231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ll=tensor(-43787.0547), avgll=tensor(-2.2419), nll=tensor(2.2419)\n"
     ]
    }
   ],
   "source": [
    "ll=0.0 # Initiating log likelihood\n",
    "n=0\n",
    "\n",
    "for w in w_test:\n",
    "    chs = ['.'] + list(w) +['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ch = ch1+ch2\n",
    "        ix1 = stoi[ch]\n",
    "        ix2 = stoi2[ch3]\n",
    "        prob = P[ix1,ix2]\n",
    "        \n",
    "        ll += torch.log(prob)\n",
    "        n+=1\n",
    "\n",
    "avgll = ll/n # average of log likelihood\n",
    "nll = -avgll \n",
    "print(f'{ll=}, {avgll=}, {nll=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668dbc52",
   "metadata": {},
   "source": [
    "### Creating probability matrix based on counting with only TRAINING words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e19f06",
   "metadata": {},
   "source": [
    "### Trigram model counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c70a8efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((729, 27), dtype=torch.int32) # 27*27 input features and 27 output features \n",
    "\n",
    "for w in w_train:\n",
    "    chs = ['.'] + list(w) +['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ch = ch1+ch2\n",
    "        ix1 = stoi[ch]\n",
    "        ix2 = stoi2[ch3]\n",
    "        \n",
    "        N[ix1, ix2] += 1\n",
    "        \n",
    "# To get probability matrix with respect to rows\n",
    "P = (N+1).float()                      # add 1 as a fake count, so that none of the combinations of bigrams, gives -- \n",
    "                                       # -- 0 count which lead to the value of infinity(log likelihood)\n",
    "# Broadcasting semantics\n",
    "P = P / P.sum(1, keepdim=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f64328d",
   "metadata": {},
   "source": [
    "### Finding the loss in validation set of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b506e8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ll=tensor(-45202.6094), avgll=tensor(-2.3264), nll=tensor(2.3264)\n"
     ]
    }
   ],
   "source": [
    "ll=0.0 # Initiating log likelihood\n",
    "n=0\n",
    "\n",
    "for w in w_val:\n",
    "    chs = ['.'] + list(w) +['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ch = ch1+ch2\n",
    "        ix1 = stoi[ch]\n",
    "        ix2 = stoi2[ch3]\n",
    "        prob = P[ix1,ix2]\n",
    "        \n",
    "        ll += torch.log(prob)\n",
    "        n+=1\n",
    "\n",
    "avgll = ll/n # average of log likelihood\n",
    "nll = -avgll \n",
    "print(f'{ll=}, {avgll=}, {nll=}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5079578c",
   "metadata": {},
   "source": [
    "### Finding the loss in test words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "96090f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ll=tensor(-45653.1797), avgll=tensor(-2.3375), nll=tensor(2.3375)\n"
     ]
    }
   ],
   "source": [
    "ll=0.0 # Initiating log likelihood\n",
    "n=0\n",
    "\n",
    "for w in w_test:\n",
    "    chs = ['.'] + list(w) +['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ch = ch1+ch2\n",
    "        ix1 = stoi[ch]\n",
    "        ix2 = stoi2[ch3]\n",
    "        prob = P[ix1,ix2]\n",
    "        \n",
    "        ll += torch.log(prob)\n",
    "        n+=1\n",
    "\n",
    "avgll = ll/n # average of log likelihood\n",
    "nll = -avgll \n",
    "print(f'{ll=}, {avgll=}, {nll=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87209e1c",
   "metadata": {},
   "source": [
    "### Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdc0596",
   "metadata": {},
   "source": [
    "#### When a probability matrix is extracted with training words based on counting, the loss of val and test sets are slightly increased to 2.3 from 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc4590f",
   "metadata": {},
   "source": [
    "### Bigram model - neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "07aa6f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "### Training with full dataset\n",
    "\n",
    "# create the dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi2[ch1]\n",
    "    ix2 = stoi2[ch2]\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b06009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the full datasets for 100 epochs - gradient descent\n",
    "for k in range(100):\n",
    "  \n",
    "  # forward pass\n",
    "  xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean() # regularization of loss\n",
    "  print(loss.item())\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef1f819",
   "metadata": {},
   "source": [
    "### Splitting the input and output features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af85faab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples:  182778\n",
      "number of validation examples:  22633\n",
      "number of test examples:  22735\n"
     ]
    }
   ],
   "source": [
    "# Training sets\n",
    "xst, yst = [], []\n",
    "for w in w_train:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi2[ch1]\n",
    "    ix2 = stoi2[ch2]\n",
    "    xst.append(ix1)\n",
    "    yst.append(ix2)\n",
    "xst = torch.tensor(xst)\n",
    "yst = torch.tensor(yst)\n",
    "num_t = xst.nelement()\n",
    "print('number of training examples: ', num_t)\n",
    "\n",
    "\n",
    "# validation sets\n",
    "xsv, ysv = [], []\n",
    "for w in w_val:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi2[ch1]\n",
    "    ix2 = stoi2[ch2]\n",
    "    xsv.append(ix1)\n",
    "    ysv.append(ix2)\n",
    "xsv = torch.tensor(xsv)\n",
    "ysv = torch.tensor(ysv)\n",
    "num_v = xsv.nelement()\n",
    "print('number of validation examples: ', num_v)\n",
    "\n",
    "\n",
    "# Test sets\n",
    "xs_test, ys_test = [], []\n",
    "for w in w_test:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi2[ch1]\n",
    "    ix2 = stoi2[ch2]\n",
    "    xs_test.append(ix1)\n",
    "    ys_test.append(ix2)\n",
    "xs_test = torch.tensor(xs_test)\n",
    "ys_test = torch.tensor(ys_test)\n",
    "num_test = xs_test.nelement()\n",
    "print('number of test examples: ', num_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d421398",
   "metadata": {},
   "source": [
    "### Loss of validation sets and test sets, when trained on full datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "144225a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5919506549835205\n"
     ]
    }
   ],
   "source": [
    "# Loss in validation sets\n",
    "\n",
    "xenc = F.one_hot(xsv, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "loss = -probs[torch.arange(num_v), ysv].log().mean() + 0.01*(W**2).mean() # regularization of loss\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7efd774a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5970656871795654\n"
     ]
    }
   ],
   "source": [
    "# Loss in test sets\n",
    "\n",
    "xenc = F.one_hot(xs_test, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "loss = -probs[torch.arange(num_test), ys_test].log().mean() + 0.01*(W**2).mean() # regularization of loss\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586e768c",
   "metadata": {},
   "source": [
    "### Training the bigram model with only training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "00302723",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)\n",
    "\n",
    "# Training the train sets for 100 epochs - gradient descent\n",
    "for k in range(100):\n",
    "  \n",
    "  # forward pass\n",
    "  xenc = F.one_hot(xst, num_classes=27).float() # input to the network: one-hot encoding\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  loss = -probs[torch.arange(num_t), yst].log().mean() + 0.01*(W**2).mean() # regularization of loss\n",
    "  #print(loss.item())\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a2ef80",
   "metadata": {},
   "source": [
    "### Loss of validation sets and test sets, when trained on only training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "dae6d403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6269145011901855\n"
     ]
    }
   ],
   "source": [
    "# Loss in validation sets\n",
    "\n",
    "xenc = F.one_hot(xsv, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "loss = -probs[torch.arange(num_v), ysv].log().mean() + 0.01*(W**2).mean() # regularization of loss\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d8fd3900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.631734609603882\n"
     ]
    }
   ],
   "source": [
    "# Loss in test sets\n",
    "\n",
    "xenc = F.one_hot(xs_test, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "loss = -probs[torch.arange(num_test), ys_test].log().mean() + 0.01*(W**2).mean() # regularization of loss\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771b5fed",
   "metadata": {},
   "source": [
    "### Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3e31e0",
   "metadata": {},
   "source": [
    "#### When a bigram model is trained with neural networks with only training sets, the loss of val and test sets are slightly increased to 2.6 from 2.5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab800ed",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning with validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "576a2edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)\n",
    "\n",
    "# Parameters to tune\n",
    "Epochs = 150\n",
    "lr =45 # learning rate\n",
    "reg_p = 0.12 # Regularization parameter\n",
    "\n",
    "\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "for k in range(Epochs):\n",
    "  \n",
    "    # forward pass calculating loss of train\n",
    "    xenc = F.one_hot(xst, num_classes=27).float() # input to the network: one-hot encoding\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    loss_t = -probs[torch.arange(num_t), yst].log().mean() + 0.01*(W**2).mean() # regularization of loss\n",
    "    \n",
    "    loss_train.append(loss_t.item())\n",
    "    \n",
    "    # calculating the loss of validation sets\n",
    "    xenc = F.one_hot(xsv, num_classes=27).float() # input to the network: one-hot encoding\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    loss_v = -probs[torch.arange(num_v), ysv].log().mean() + reg_p*(W**2).mean() # regularization of loss\n",
    "    \n",
    "    loss_val.append(loss_v.item())\n",
    "  \n",
    "    # backward pass\n",
    "    W.grad = None # set to zero the gradient\n",
    "    loss_v.backward()\n",
    "  \n",
    "    # update\n",
    "    W.data += -lr * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9db0f4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1320caf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fa351c4e2f0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_train, label='train loss')\n",
    "plt.plot(loss_val, label='val loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69cb3db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5688018798828125\n"
     ]
    }
   ],
   "source": [
    "# Loss in test sets\n",
    "\n",
    "xenc = F.one_hot(xs_test, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "loss = -probs[torch.arange(num_test), ys_test].log().mean() + 0.01*(W**2).mean() # regularization of loss\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fefa01b",
   "metadata": {},
   "source": [
    "### Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabce7d6",
   "metadata": {},
   "source": [
    "#### Best test loss = 2.56, on tuning hyperparameters - learning rate = 45, regularization parameter = 0.12, Epochs = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842852f9",
   "metadata": {},
   "source": [
    "### Removing one-hot encoded by indexing to find loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d480ef7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5688018798828125\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "logits = torch.zeros((len(xs_test), 27))\n",
    "\n",
    "logits = W[xs_test]\n",
    "        \n",
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "loss = -probs[torch.arange(num_test), ys_test].log().mean() + 0.01*(W**2).mean() # regularization of loss\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aea50bd",
   "metadata": {},
   "source": [
    "#### Using one hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8febcffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7960,  1.6383,  0.4766,  ..., -1.1579, -0.3531,  0.0841],\n",
       "        [ 0.4365,  0.1629, -0.0499,  ..., -0.8476,  0.9273, -0.5505],\n",
       "        [ 2.0491,  1.9108, -1.0519,  ..., -1.1754,  0.9324, -0.8856],\n",
       "        ...,\n",
       "        [ 1.9303,  1.5593, -0.8830,  ..., -0.5812, -0.7501, -0.2329],\n",
       "        [ 1.1820,  2.6159, -0.3737,  ..., -0.4006,  0.7600, -0.1605],\n",
       "        [ 1.3780,  0.9318, -0.3305,  ...,  0.5287,  0.1072,  0.3227]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc = F.one_hot(xs_test, num_classes=27).float()\n",
    "logits1 = xenc @ W\n",
    "logits1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8aa15c",
   "metadata": {},
   "source": [
    "#### Using Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c1c24f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7960,  1.6383,  0.4766,  ..., -1.1579, -0.3531,  0.0841],\n",
       "        [ 0.4365,  0.1629, -0.0499,  ..., -0.8476,  0.9273, -0.5505],\n",
       "        [ 2.0491,  1.9108, -1.0519,  ..., -1.1754,  0.9324, -0.8856],\n",
       "        ...,\n",
       "        [ 1.9303,  1.5593, -0.8830,  ..., -0.5812, -0.7501, -0.2329],\n",
       "        [ 1.1820,  2.6159, -0.3737,  ..., -0.4006,  0.7600, -0.1605],\n",
       "        [ 1.3780,  0.9318, -0.3305,  ...,  0.5287,  0.1072,  0.3227]],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "logits2 = torch.zeros((len(xs_test), 27))\n",
    "\n",
    "logits2 = W[xs_test[np.arange(len(xs_test))]]\n",
    "logits2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "064b5cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  1, 18,  ..., 25, 26, 24]), torch.Size([22735]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs_test, xs_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37e0ae3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.7960,  1.6383,  0.4766,  ..., -1.1579, -0.3531,  0.0841],\n",
       "         [ 0.4365,  0.1629, -0.0499,  ..., -0.8476,  0.9273, -0.5505],\n",
       "         [ 2.0491,  1.9108, -1.0519,  ..., -1.1754,  0.9324, -0.8856],\n",
       "         ...,\n",
       "         [ 1.9303,  1.5593, -0.8830,  ..., -0.5812, -0.7501, -0.2329],\n",
       "         [ 1.1820,  2.6159, -0.3737,  ..., -0.4006,  0.7600, -0.1605],\n",
       "         [ 1.3780,  0.9318, -0.3305,  ...,  0.5287,  0.1072,  0.3227]],\n",
       "        grad_fn=<IndexBackward0>),\n",
       " torch.Size([22735, 27]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[xs_test], W[xs_test].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "754d00bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.7960,  1.6383,  0.4766,  0.5254,  0.8850,  0.3950, -0.6683, -0.2648,\n",
       "        -0.0241, -0.3685,  1.1775,  1.2942,  0.3637,  0.8850,  0.2640, -0.3229,\n",
       "        -0.3229, -1.3517,  0.7006,  0.8203,  0.6535, -1.3172, -0.6287, -0.6483,\n",
       "        -1.1579, -0.3531,  0.0841], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e09ded9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.7960,  1.6383,  0.4766,  0.5254,  0.8850,  0.3950, -0.6683, -0.2648,\n",
       "        -0.0241, -0.3685,  1.1775,  1.2942,  0.3637,  0.8850,  0.2640, -0.3229,\n",
       "        -0.3229, -1.3517,  0.7006,  0.8203,  0.6535, -1.3172, -0.6287, -0.6483,\n",
       "        -1.1579, -0.3531,  0.0841], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774ffb61",
   "metadata": {},
   "source": [
    "### Using F.crossentropy instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1fda1246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5688023567199707\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Loss in test sets\n",
    "\n",
    "xenc = F.one_hot(xs_test, num_classes=27).float() # input to the network: one-hot encoding\n",
    "logits = xenc @ W # predict log-counts\n",
    "loss = F.cross_entropy(logits, ys_test) + 0.01*(W**2).mean() # regularization of loss\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8289fd",
   "metadata": {},
   "source": [
    "#### Finding loss without F.crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5306b2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5614, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = logits.exp() # counts, equivalent to N\n",
    "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "\n",
    "loss1 = -probs[torch.arange(num_test), ys_test].log().mean()\n",
    "loss1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6168dab",
   "metadata": {},
   "source": [
    "#### With Cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8efcf451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5614, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss2 = F.cross_entropy(logits, ys_test)\n",
    "loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca449795",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
