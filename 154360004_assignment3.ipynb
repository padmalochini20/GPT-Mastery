{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "G3sQc-4VQh17"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download the names.txt file from github\n",
        "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yah02rR8Ru-v",
        "outputId": "961d89a8-03f5-463a-e47b-50bc339e3911"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-08 16:01:08--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt.2’\n",
            "\n",
            "\rnames.txt.2           0%[                    ]       0  --.-KB/s               \rnames.txt.2         100%[===================>] 222.80K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-08-08 16:01:08 (6.09 MB/s) - ‘names.txt.2’ saved [228145/228145]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read in all the words\n",
        "words = open('names.txt', 'r').read().splitlines()\n",
        "words[:8]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjDQmXXBR4Eh",
        "outputId": "f0402e0e-dd4f-4a9b-b609-e7194adcc460"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the vocabulary of characters and mappings to/from integers\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "print(itos)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BrNmv0YR34y",
        "outputId": "d0790b14-89c8-4500-cfc4-45a4cd7a2fcc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the dataset\n",
        "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
        "\n",
        "def build_dataset(words):\n",
        "  X, Y = [], []\n",
        "\n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      context = context[1:] + [ix] # crop and append\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape, Y.shape)\n",
        "  return X, Y\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)    # shuffling the order of words before splitting\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
        "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
      ],
      "metadata": {
        "id": "IMb6a1hOR3ui",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbc5733a-7351-42d7-a8b0-011c44a3ae56"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([182625, 3]) torch.Size([182625])\n",
            "torch.Size([22655, 3]) torch.Size([22655])\n",
            "torch.Size([22866, 3]) torch.Size([22866])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C = torch.randn((27, 10), generator=g)\n",
        "W1 = torch.randn((30, 200), generator=g)\n",
        "b1 = torch.randn(200, generator=g)\n",
        "W2 = torch.randn((200, 27), generator=g)\n",
        "b2 = torch.randn(27, generator=g)\n",
        "parameters = [C, W1, b1, W2, b2]"
      ],
      "metadata": {
        "id": "tQBw4bWjR3jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(p.nelement() for p in parameters) # number of parameters in total"
      ],
      "metadata": {
        "id": "30MtkZWER3aE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1629d16-331e-41db-b6a0-81b427383dce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11897"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "id": "zMsBUysHR3Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(20000):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (32,))\n",
        "\n",
        "  # forward pass\n",
        "  emb = C[Xtr[ix]] # (32, 3, 2)\n",
        "  h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100)\n",
        "  logits = h @ W2 + b2 # (32, 27)\n",
        "  loss = F.cross_entropy(logits, Ytr[ix])\n",
        "  #print(loss.item())\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  #lr = lrs[i]\n",
        "  lr = 0.1 if i < 10000 else 0.01\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # # track stats\n",
        "  # #lri.append(lre[i])\n",
        "  # stepi.append(i)\n",
        "  # lossi.append(loss.log10().item())\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "id": "cEayPm5gR3B4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9ed74ed-84af-4799-e90b-28c776451081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.498176336288452\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# validation loss\n",
        "emb = C[Xdev] # (32, 3, 2)\n",
        "h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100)\n",
        "logits = h @ W2 + b2 # (32, 27)\n",
        "loss = F.cross_entropy(logits, Ydev)\n",
        "print('validation loss',loss.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TFjk3_TXq3V",
        "outputId": "a84233da-0dbf-49ce-bd07-4ff86c33503a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation loss tensor(2.3188)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### E01: Tune the hyperparameters of the training to beat my best validation loss of 2.2"
      ],
      "metadata": {
        "id": "_9WYxcnxQuc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Increase the hidden layers from 200 to 400\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C = torch.randn((27, 10), generator=g)\n",
        "W1 = torch.randn((30, 400), generator=g)\n",
        "b1 = torch.randn(400, generator=g)\n",
        "W2 = torch.randn((400, 27), generator=g)\n",
        "b2 = torch.randn(27, generator=g)\n",
        "parameters = [C, W1, b1, W2, b2]\n",
        "\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "id": "83N7aO9VWqxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(300000):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (32,))\n",
        "\n",
        "  # forward pass\n",
        "  emb = C[Xtr[ix]] # (32, 3, 2)\n",
        "  h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100)\n",
        "  logits = h @ W2 + b2 # (32, 27)\n",
        "  loss = F.cross_entropy(logits, Ytr[ix])\n",
        "  #print(loss.item())\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  #lr = lrs[i]\n",
        "  #lr = 0.1 if i < 10000 else 0.01\n",
        "  lr = 0.001\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # # track stats\n",
        "  # #lri.append(lre[i])\n",
        "  # stepi.append(i)\n",
        "  # lossi.append(loss.log10().item())\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlAnf54FXJnR",
        "outputId": "b6f745c4-7a93-4a8c-86e3-f4fa00dd3b39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.771917462348938\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# validation loss\n",
        "emb = C[Xdev] # (32, 3, 2)\n",
        "h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100)\n",
        "logits = h @ W2 + b2 # (32, 27)\n",
        "loss = F.cross_entropy(logits, Ydev)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59LNJOY4XPzG",
        "outputId": "d025e4cd-4532-4985-d1a1-91e7551da33a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.3468, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Comments/Insights: Increasing number of neurons in a single hidden layer, reduces the training loss but does not reduce the validation loss, which means our model is overfitting. Next will try with multilayer neural networks."
      ],
      "metadata": {
        "id": "jZg9b6sIYa0t"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nKrFGrm3dTUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wISa9H4bdTJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TzQsDRE_dTAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jw6UIW7zdS1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pOsPR2lldSke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Include an additional hidden layers\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C = torch.randn((27, 10), generator=g)\n",
        "W1 = torch.randn((30, 200), generator=g)\n",
        "b1 = torch.randn(200, generator=g)\n",
        "W2 = torch.randn((200, 200), generator=g)\n",
        "b2 = torch.randn(200, generator=g)\n",
        "W3 = torch.randn((200, 200), generator=g)\n",
        "b3 = torch.randn(200, generator=g)\n",
        "W4 = torch.randn((200, 27), generator=g)\n",
        "b4 = torch.randn(27, generator=g)\n",
        "parameters = [C, W1, b1, W2, b2, W3, b3, W4, b4]\n",
        "\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "id": "m-5z_S7HXPuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(50000):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (32,))\n",
        "\n",
        "  # forward pass\n",
        "  emb = C[Xtr[ix]] # (32, 3, 10)\n",
        "  h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # 32x30 @ 30x300 -> 32 x 300\n",
        "  logits1 = torch.tanh(h @ W2 + b2) # 32x300 @ 300x300 -> 32 x 300\n",
        "  logits2 = torch.tanh(logits1 @ W3 + b3) # 32x300 @ 300x300 -> 32 x 300\n",
        "  logits = logits2 @ W4 + b4 # 32x300 @ 300x27 -> 32 x 27\n",
        "  loss = F.cross_entropy(logits, Ytr[ix])\n",
        "  #print(loss.item())\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  #lr = lrs[i]\n",
        "  lr = 0.15 if i < 30000 else 0.001\n",
        "  #lr = 0.001\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # # track stats\n",
        "  # #lri.append(lre[i])\n",
        "  # stepi.append(i)\n",
        "  # lossi.append(loss.log10().item())\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtE-LI5oXPoT",
        "outputId": "2a05b34c-ffa1-43d3-d713-0f4030f7b88c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0119032859802246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# validation loss\n",
        "emb = C[Xdev]\n",
        "h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # 32x30 @ 30x300 -> 32 x 300\n",
        "logits1 = torch.tanh(h @ W2 + b2) # 32x300 @ 300x300 -> 32 x 300\n",
        "logits2 = torch.tanh(logits1 @ W3 + b3) # 32x300 @ 300x300 -> 32 x 300\n",
        "logits = logits2 @ W4 + b4 # 32x300 @ 300x27 -> 32 x 27\n",
        "loss = F.cross_entropy(logits, Ydev)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Fttaf4GXPi1",
        "outputId": "ac348082-aad6-4239-83d2-e0fc91f5214f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.1920, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Comments: Creating multilayer perceptrons and substantially decreasing learning rate reduces loss of validation set to 2.19"
      ],
      "metadata": {
        "id": "p02LVAlcp6eq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LBzqmgaoXPdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7lnQfad3XPXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### E02: I was not careful with the intialization of the network in this video. (1) What is the loss you'd get if the predicted probabilities at initialization were perfectly uniform? What loss do we achieve? (2) Can you tune the initialization to get a starting loss that is much more similar to (1)?"
      ],
      "metadata": {
        "id": "HyhKkTPumPK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C = torch.randn((27, 10), generator=g)\n",
        "W1 = torch.randn((30, 200), generator=g)\n",
        "b1 = torch.randn(200, generator=g)\n",
        "W2 = torch.randn((200, 27), generator=g)\n",
        "b2 = torch.randn(27, generator=g)\n",
        "parameters = [C, W1, b1, W2, b2]"
      ],
      "metadata": {
        "id": "9iNStUpimT2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "id": "BPLc5bwLmnXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(30000):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (32,))\n",
        "\n",
        "  # forward pass\n",
        "  emb = C[Xtr[ix]] # (32, 3, 2)\n",
        "  h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100)\n",
        "  logits = h @ W2 + b2 # (32, 27)\n",
        "  loss = F.cross_entropy(logits, Ytr[ix])\n",
        "  print(loss.item())\n",
        "\n",
        "  break\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  #lr = lrs[i]\n",
        "  #lr = 0.1 if i < 10000 else 0.01\n",
        "  lr = 0.001\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # # track stats\n",
        "  # #lri.append(lre[i])\n",
        "  # stepi.append(i)\n",
        "  # lossi.append(loss.log10().item())\n",
        "\n",
        "#print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSdDbeHtmskD",
        "outputId": "cc966402-f282-44ee-91ce-6a465097c57f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22.844247817993164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Initial loss is very high 22.84, trying to reduce initial loss by adjusting the weights to approximately uniform."
      ],
      "metadata": {
        "id": "cjdsh4vRqZvj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bWn_N2UKmser"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Making the neuron weights to approximately uniform by multiply it with 0.02 and bias as 0 initially\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C = torch.randn((27, 10), generator=g)\n",
        "W1 = torch.randn((30, 200), generator=g) * 0.02\n",
        "b1 = torch.randn(200, generator=g) * 0\n",
        "W2 = torch.randn((200, 27), generator=g) * 0.02\n",
        "b2 = torch.randn(27, generator=g) * 0\n",
        "parameters = [C, W1, b1, W2, b2]\n",
        "\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "id": "nPCQy_pcmsYz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(30000):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (32,))\n",
        "\n",
        "  # forward pass\n",
        "  emb = C[Xtr[ix]] # (32, 3, 2)\n",
        "  h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100)\n",
        "  logits = h @ W2 + b2 # (32, 27)\n",
        "  loss = F.cross_entropy(logits, Ytr[ix])\n",
        "  print(loss.item())\n",
        "\n",
        "  break\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  #lr = lrs[i]\n",
        "  #lr = 0.1 if i < 10000 else 0.01\n",
        "  lr = 0.001\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # # track stats\n",
        "  # #lri.append(lre[i])\n",
        "  # stepi.append(i)\n",
        "  # lossi.append(loss.log10().item())\n",
        "\n",
        "#print(loss.item())"
      ],
      "metadata": {
        "id": "oJmm9z8fmsRy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "246b8587-d6ba-46e9-9e71-79a4eb22985d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.296635627746582\n",
            "3.296635627746582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Insights: Making the initial weights to nearly uniform reduces the initial loss from 22.844 to 3.296"
      ],
      "metadata": {
        "id": "HmihiTY7uald"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LUsVGzxWmsJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zrwvI-IsmsEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### E01: I did not get around to seeing what happens when you initialize all weights and biases to zero. Try this and train the neural net. You might think either that 1) the network trains just fine or 2) the network doesn't train at all, but actually it is 3) the network trains but only partially, and achieves a pretty bad final performance. Inspect the gradients and activations to figure out what is happening and why the network is only partially training, and what part is being trained exactly."
      ],
      "metadata": {
        "id": "x-UhL6l-ou__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Making the neuron weights to approximately uniform by multiply it with 0.02 and bias as 0 initially\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C = torch.randn((27, 10), generator=g)\n",
        "W1 = torch.randn((30, 200), generator=g) * 0.02\n",
        "b1 = torch.randn(200, generator=g) * 0\n",
        "W2 = torch.randn((200, 27), generator=g) * 0.02\n",
        "b2 = torch.randn(27, generator=g) * 0\n",
        "parameters = [C, W1, b1, W2, b2]\n",
        "\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "id": "YUHaP7WRooC-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(30000):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (32,))\n",
        "\n",
        "  # forward pass\n",
        "  emb = C[Xtr[ix]] # (32, 3, 2)\n",
        "  h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100)\n",
        "  logits = h @ W2 + b2 # (32, 27)\n",
        "  loss = F.cross_entropy(logits, Ytr[ix])\n",
        "  #print(loss.item())\n",
        "\n",
        "  #break\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  #lr = lrs[i]\n",
        "  #lr = 0.1 if i < 10000 else 0.01\n",
        "  lr = 0.01\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # # track stats\n",
        "  # #lri.append(lre[i])\n",
        "  # stepi.append(i)\n",
        "  # lossi.append(loss.log10().item())\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "id": "fYWmomONon69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbf83558-aea8-44bd-c2c4-aba7b0154c02"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5573537349700928\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# validation loss\n",
        "emb = C[Xdev] # (32, 3, 2)\n",
        "h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100)\n",
        "logits = h @ W2 + b2 # (32, 27)\n",
        "loss = F.cross_entropy(logits, Ydev)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwTjPTcfv3kj",
        "outputId": "3315d90d-5929-42af-dbba-af0ccc8f70e3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.3422, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1dPPQGhvvaRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Weights and bias to 0"
      ],
      "metadata": {
        "id": "8UMNwh3mva4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Making the neuron weights and bias to 0 initially\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C = torch.randn((27, 10), generator=g)\n",
        "W1 = torch.randn((30, 200), generator=g) * 0\n",
        "b1 = torch.randn(200, generator=g) * 0\n",
        "W2 = torch.randn((200, 27), generator=g) * 0\n",
        "b2 = torch.randn(27, generator=g) * 0\n",
        "parameters = [C, W1, b1, W2, b2]\n",
        "\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "id": "2HBj5JZPon1r"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(30000):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (32,))\n",
        "\n",
        "  # forward pass\n",
        "  emb = C[Xtr[ix]] # (32, 3, 2)\n",
        "  h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100)\n",
        "  logits = h @ W2 + b2 # (32, 27)\n",
        "  loss = F.cross_entropy(logits, Ytr[ix])\n",
        "  #print(loss.item())\n",
        "\n",
        "  #break\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  #lr = lrs[i]\n",
        "  #lr = 0.1 if i < 10000 else 0.01\n",
        "  lr = 0.01\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # # track stats\n",
        "  # #lri.append(lre[i])\n",
        "  # stepi.append(i)\n",
        "  # lossi.append(loss.log10().item())\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "id": "MiGdlwFEonvN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca0d3b65-a9ab-4414-82ea-f8860f1f4265"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8362462520599365\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# validation loss\n",
        "emb = C[Xdev] # (32, 3, 2)\n",
        "h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100)\n",
        "logits = h @ W2 + b2 # (32, 27)\n",
        "loss = F.cross_entropy(logits, Ydev)\n",
        "loss"
      ],
      "metadata": {
        "id": "NJWsd-uLonpZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00f8fc3a-83be-44ef-bdc2-118e77a2b73b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.8227, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Although there is a small difference in loss, it looks like despite of weights and bias being initialized to 0, the model trains. However as mentioned in the lecture, it just partially trains."
      ],
      "metadata": {
        "id": "LI4wccXJwidz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zR6NGOLnonje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wim3NAP8onbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### E02: BatchNorm, unlike other normalization layers like LayerNorm/GroupNorm etc. has the big advantage that after training, the batchnorm gamma/beta can be \"folded into\" the weights of the preceeding Linear layers, effectively erasing the need to forward it at test time. Set up a small 3-layer MLP with batchnorms, train the network, then \"fold\" the batchnorm gamma/beta into the preceeding Linear layer's W,b by creating a new W2, b2 and erasing the batch norm. Verify that this gives the same forward pass during inference. i.e. we see that the batchnorm is there just for stabilizing the training, and can be thrown out after training is done! pretty cool."
      ],
      "metadata": {
        "id": "Iu2qOBX8wT-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# With gamma and beta\n",
        "vocab_size = 27\n",
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) #* 0.2\n",
        "#b1 = torch.randn(n_hidden,                        generator=g) * 0.01\n",
        "W3 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
        "b3 = torch.randn(vocab_size,                      generator=g) * 0\n",
        "\n",
        "# BatchNorm parameters\n",
        "gamma = torch.ones((1, n_hidden))   # gamma\n",
        "beta = torch.zeros((1, n_hidden))  # beta\n",
        "# bnmean_running = torch.zeros((1, n_hidden))\n",
        "# bnstd_running = torch.ones((1, n_hidden))\n",
        "\n",
        "parameters = [C, W1, W3, b3, gamma, beta]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n"
      ],
      "metadata": {
        "id": "8JgjEMaRonR1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c158af66-03da-4ad6-df7b-08d3a831881a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# same optimization as last time\n",
        "max_steps = 2000\n",
        "batch_size = 32\n",
        "lossi = []\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "  # forward pass\n",
        "  emb = C[Xb] # embed the characters into vectors\n",
        "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "  # Linear layer\n",
        "  hpreact = embcat @ W1 #+ b1 # hidden layer pre-activation\n",
        "  # BatchNorm layer\n",
        "  # -------------------------------------------------------------\n",
        "  bnmeani = hpreact.mean(0, keepdim=True)\n",
        "  bnstdi = hpreact.std(0, keepdim=True)\n",
        "  hpreact = gamma * (hpreact - bnmeani) / bnstdi + beta\n",
        "  # with torch.no_grad():\n",
        "  #   bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
        "  #   bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
        "  # -------------------------------------------------------------\n",
        "  # Non-linearity\n",
        "  h = torch.tanh(hpreact) # hidden layer\n",
        "  logits = h @ W3 + b3 # output layer\n",
        "  loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  #lr = 0.1 if i < 10000 else 0.01 # step learning rate decay\n",
        "  lr = 0.1\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # track stats\n",
        "  if i % 10000 == 0: # print every once in a while\n",
        "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "  lossi.append(loss.log10().item())\n",
        "\n",
        "#print(loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s20B4SkVw-zY",
        "outputId": "44148966-875e-42cc-ec12-c552526071d0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      0/   2000: 3.3239\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LRj-7FYBw-vN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Replacing gamma and beta with another layers with W2 and b2"
      ],
      "metadata": {
        "id": "jldRHlGl3x92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# With gamma and beta\n",
        "vocab_size = 27\n",
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) #* 0.2\n",
        "#b1 = torch.randn(n_hidden,                        generator=g) * 0.01\n",
        "W3 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
        "b3 = torch.randn(vocab_size,                      generator=g) * 0\n",
        "\n",
        "# BatchNorm parameters\n",
        "W2 = torch.ones((n_hidden, n_hidden))   # gamma\n",
        "b2 = torch.zeros((1, n_hidden))  # beta\n",
        "\n",
        "\n",
        "parameters = [C, W1, W3, b3, W2, b2]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXF9KNavw-rk",
        "outputId": "22cbc926-1ea5-4581-a3db-941ede8c0d58"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51897\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# same optimization as last time\n",
        "max_steps = 2000\n",
        "batch_size = 32\n",
        "lossi = []\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "  # forward pass\n",
        "  emb = C[Xb] # embed the characters into vectors\n",
        "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "  # Linear layer\n",
        "  hpreact = embcat @ W1 #+ b1 # hidden layer pre-activation\n",
        "  # BatchNorm layer\n",
        "  # -------------------------------------------------------------\n",
        "  bnmeani = hpreact.mean(0, keepdim=True)\n",
        "  bnstdi = hpreact.std(0, keepdim=True)\n",
        "\n",
        "  hpreact = (hpreact - bnmeani) / bnstdi\n",
        "\n",
        "\n",
        "  # -------------------------------------------------------------\n",
        "  # Non-linearity\n",
        "  #h = torch.tanh(hpreact) # hidden layer\n",
        "  h = torch.tanh(hpreact @ W2 +b2)\n",
        "\n",
        "  logits = h @ W3 + b3 # output layer\n",
        "  loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  #lr = 0.1 if i < 10000 else 0.01 # step learning rate decay\n",
        "  lr = 0.1\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # track stats\n",
        "  if i % 10000 == 0: # print every once in a while\n",
        "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "  lossi.append(loss.log10().item())\n",
        "\n",
        "#print(loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tj36nUiRw-nn",
        "outputId": "87a55a34-17a3-4075-a20b-6db9d36ec099"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      0/   2000: 3.3220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4-3ucJTXw-jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Both cases give approximately similar loss in forward pass, which means batch norm is just for stabilizing the activations."
      ],
      "metadata": {
        "id": "wiEZxhak5v-N"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kgrghkJhw-fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "omLU7o75w-bY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TX_lWGQWw-Xr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}